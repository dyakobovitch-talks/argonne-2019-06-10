{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Numpy \n",
    "\n",
    "We're going to start off with a foray into the `Numpy` library, which is one of the fundamental packages for scientific computing in Python. It turns out that the `Pandas DataFrames` we worked with last class are actually built off the `numpy array` (which we'll get to), so it's important to have some basic knowldege of what's running under the hood of our `DataFrames`. We started with `DataFrames` as opposed to `numpy arrays` because they are a little bit more intuitive and we're able to interact with them from a higher level (this is largely due to the ability to label our data). \n",
    "\n",
    "While `Numpy` offers an amazing amount of functionality (see the [docs](http://www.numpy.org/) for a better idea), one of it's mainstays is the `numpy array` (an n-dimensional array), which is what we'll focus on. There are loads of things that you can do with `numpy arrays`, and we're going to introduce some of their amazing capabilities. Learning about everything `numpy arrays` can do really just takes working with them day in and day out, and so we'll try to aim for breadth over depth. We want you to walk out of here with an idea of the many things you can do with `numpy` and `numpy arrays`, and the types of situations where you would want to use them. \n",
    "\n",
    "## The basics of the Array\n",
    "\n",
    "#### What's the big deal with Numpy Arrays?\n",
    "\n",
    "What's so special about a `numpy array`? From a high level, they are kind of like lists - they just store a bunch of stuff in a container. It turns out, though, that a numpy array is much faster to interact with and perform certain types of calculations with than a standard Python list. Why is that, though? The two main reasons that they are faster are: \n",
    "\n",
    "1. They are stored as one contiguous block of memory, rather than being spread out across multiple locations like a list. \n",
    "2. Each item in a numpy array is of the same data type (i.e. all integers, all floats, etc.), rather than a conglomerate of any number of data types (as a list is). We call this idea homogeneity, as opposed to the possible heterogeneity of Python lists.\n",
    "\n",
    "Just how much faster are they? Let's take the numbers from 0 to 1 million, and sum those numbers, timing it with both a list and a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # Standard alias when importing numpy - follow this convention!\n",
    "numpy_array = np.arange(0, 1000000)\n",
    "python_list = range(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.sum(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sum(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sum(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, numpy runs nearly an order of magnitude faster! This is because of those two points above. Because numpy arrays store data in contiguous blocks of memory, they are able to take advantage of **vectorization**, which is the ability of a CPU to perform one operation on mulitiple pieces of data at once. In addition, since a numpy array knows what type each object it is storing is (and those types don't change), it doesn't have to waste time checking what type each object is (like a list). The combo of these two things speeds up our calcualtion quite a bit.\n",
    "\n",
    "Notice, too, that we had to specify `np.sum()` - numpy's implementation of sum. When we just used the built-in Python `sum()` on the numpy array, the calculation was actually slower! This is because numpy arrays are optimized for vectorized computations, and when we try to do a non-vectorized operation we pay a price. \n",
    "\n",
    "It's also worth nothing that all we did above was a sum - just a **simple** sum. When we move to doing more complicated operations, we'll save even more time! Let's look at what else numpy arrays can do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a Numpy Array\n",
    "\n",
    "Now that we know how awesome `numpy arrays` can be, let's dive into them. We're not going to cover everything that you can do with `numpy arrays` (see the [methods docs](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ndarray.html#numpy.ndarray) for that), but we'll look at the basics. \n",
    "\n",
    "Let's start with how we can create a `numpy array`. To do this, we use the `np.array()` constructor, which expects some kind of array or something that exposes the array interface (i.e. acts like an array - lists and tuples are examples). So, this means that we can create a numpy array by passing in a list or tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lst_ndarray = np.array([1, 2, 3, 4, 5])\n",
    "my_tuple_ndarray = np.array((1, 2, 3, 4, 5), np.int32) # You can specify the data type \n",
    "                                                       # upon creation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just like we have the shape attribute on Pandas DataFrames, we also have it on numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_lst_ndarray.shape)\n",
    "print(my_tuple_ndarray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also have the dtype attribute, which will tell us the data type of the objects in our ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_lst_ndarray.dtype)\n",
    "print(my_tuple_ndarray.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lst_ndarray2 = np.array([\"1\", 2, 3, \"10\", 5])\n",
    "print(my_lst_ndarray2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you try to tell the ndarray to be a certain data type, it will try to cast every object you pass in to that data type (here a 32-bit integer), and fail if it can't cast it to that data type. Below, we are able to cast \"10\" to a 32-bit integer, so this is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lst_ndarray3 = np.array([1, 2, 3, \"10\", 5], np.int32) \n",
    "print(my_lst_ndarray3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work, because Python can't cast the string 'bozo' as a 32 bit integer. \n",
    "my_lst_ndarray3 = np.array([1, 2, 3, \"bozo\", 5], np.int32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are some other methods of constructing a numpy array. It's helpful to know these exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_arr = np.zeros((3,4)) # Create a matrix of zeros with 3 rows and 4 columns. \n",
    "ones_arr = np.ones((10,20))  # Create a matrix of ones with 10 rows and 20 columns.\n",
    "identity_arr = np.identity(50) # Create an identity matrix with 50 rows and 50 columns. \n",
    "random_arr = np.random.rand(2, 2) # Create a 2x2 array of random floats ranging from 0 to 1. \n",
    "range_arr = np.arange(0, 20, 0.5) # Create a numpy array with arguments (start, end, step_size). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Array Math\n",
    "\n",
    "When working with a `numpy` array, we have all of the basic mathematics operators available to us - `+`, `-`, `*`, `/`, `**`, and `%`. Frequently, we'll be working with two arrays that are the same size, in which case these operators will just be performed **element-wise**. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr = np.array([1, 2, 3, 4])\n",
    "second_arr = np.array([5, 6, 7, 8])\n",
    "first_arr + second_arr # Each element is lined up with it's corresponding element in the other \n",
    "                       # array, and the addition is then performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr = np.array([[1, 2], [3, 4]]) # This is now a two-dimensional array. \n",
    "second_arr = np.array([[5, 6], [7, 8]]) # This is now a two-dimensional array. \n",
    "first_arr * second_arr # Each element is lined up with it's corresponding element in the other \n",
    "                       # array, and the multiplication is then performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that our numerical operations can also work when we want to perform an operation between a `numpy array` and a single value. For example, let's say that we want to subtract `4` from `first_arr` above, or multiply it by `5`, or find the remainder when everything is divided by `3`. We can do that via the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr = np.array([[1, 2], [3, 4]]) # This is now a two-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr % 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept that allows this to happen is referred to as [broadcasting](http://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html). It is a concept that will be particularly useful when working with and interacting with `numpy` arrays. Basically, it takes that single number on the right (the `4`, `5`, or `3` above), and **broadcasts** it's shape to match that of `first_arr` (`2 x 2`). After doing so, it then performs the operation element-wise like we saw before. \n",
    "\n",
    "It turns out that things can get a little more intricate than this. If we wanted, we could perform mathematical operations like the above at a column level, or row level. For example, we could subtract off `4` from the first column and `5` from the second column, or `4` from the first row and `5` from the second row. We would do that via the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_arr = np.array([[1, 2], [3, 4]]) # This is now a two-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr - [4, 5] # Here, we subtract 4 off the first **column** and 5 off the second **column**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_arr - [[4], [5]] # Here, we subtract 4 from the first **row** and 5 from the second **row**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Little bit more of Numpy Arrays\n",
    "\n",
    "Okay, so now that we know a little bit about a `numpy array`, what else can we do with it? There are actually quite a number of things we can do. We can index into them, perform calculations, ask for aggregation type metrics, etc. \n",
    "\n",
    "#### Indexing \n",
    "\n",
    "Let's begin by indexing into them. With `numpy arrays`, we don't have the `.loc[]`, `.iloc[]`, or `.ix[]` methods like we do on a DataFrame - we simply index into them like we would a list. It's effectively a multidimensional list, though. Therefore, we can pass it multiple indexing values. Let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape will reshape the data to the shape that you tell it to (here 5 rows, 4 columns). \n",
    "range_arr = np.arange(0, 20, 1).reshape(5, 4)\n",
    "range_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr[:, 2] # Grab every row, but only the element at index 2 in those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr[0:2] # With no second index, this defaults to taking the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr[0:2, 1:3] # The first set of numbers refers to the rows to \n",
    "                    # grab, the second set the columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other methods\n",
    "\n",
    "Now let's look at some of the other methods that are available. Again, there is a ton we can do, and we're aiming here to at least get your eyes on a lot of the things that are possible. We also want to give you a notebook here that you can look back at to see what is possible (Google is also amazing for this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember what this looks like. \n",
    "range_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can perform sums in any direction with a method on the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.sum(axis=0) # Sum along the rows (i.e. get column totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.sum(axis=1) # Sum along the columns (i.e. get row totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.sum() # Get sum of all elements in numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also grab the mean, standard deviation, max, and min values along the rows (i.e. for the columns). We could also do this along the columns, or for the array as a whole (just like we did with `.sum()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to instead grab the **index** at which those min and max values occur (either along the rows or columns), then we can use the `argmin()` and `argmax()` methods available on our numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.argmin(axis=0) # We see that the mins of each column occur at row 1 (index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.argmax(axis=0) # We see that the maxes of each column occur at row 5 (index 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.argmin() # Here we get the index of the overall minimum (the 0th index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.argmax() # Here we get the index of the overall maximum (the last index). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can get the cumulative sum or product with the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.cumsum(axis=0)  # Here it gets the cumsum along the rows (i.e. from top to bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr.cumprod(axis=0) # Gets the cumprod along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can flatten our arrays as follows. \n",
    "range_arr.flatten()\n",
    "range_arr.ravel()  # They look the same in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief look at a cool Numpy method\n",
    "\n",
    "The majority, if not all, of the methods that we looked at for numpy arrays are available on Pandas columns. They might have some slightly different naming conventions (`idxmax` on a column versus `argmax` on a numpy array, for example), but since Pandas DataFrames are built on numpy arrays, the methods available on numpy arrays largely coincide with the methods available on Pandas DataFrames. \n",
    "\n",
    "Many of these methods are available as functions on the `numpy` module itself, as well. Just like we can call the `argmax()` method on a numpy array, we can call `np.argmax()` and pass in a list or tuple. Before we move back to DataFrames, let's look at one last method that is available in `numpy`, `np.where()`. `np.where()` can help us to find what elements in a numpy array meet some condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_ndarray = np.array([2, 4, 6, 8, 24, 3, 8, 9, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(my_ndarray <= 2)) # Returns the indices where the data meet the condition. \n",
    "print(np.where(my_ndarray == 8)) # Returns the indices where the data meet the condition. \n",
    "print(np.where(my_ndarray > 6)) # Returns the indices where the data meet the condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets with Pandas \n",
    "\n",
    "Pandas functions that allow us to combine two sets of data include the use of `pd.merge()`, `df.join()`, `df.merge()`, and `pd.concat()`. For the most part, these do largely the same things (although you'll notice the slight syntax difference with `merge()` and `concat()` being able to be called via the Pandas module and `merge` and `join()` being able to be called on a DataFrame instance). There are some cases where one of these might be better than another in terms of writing less code or performing some kind of data combination in an easier way. The major differences between these, though, largely depend on what they do by default when you try to combine different data. By default, `merge()` looks to join on common columns, `join()` on common indices, and `concat()` by just appending on a given axis.\n",
    "\n",
    "You can find more detail about the differences between all three of these in the [docs](http://pandas.pydata.org/pandas-docs/stable/merging.html). We'll look at some examples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll go back to our wine data set. Who doesn't love wine?\n",
    "import pandas as pd\n",
    "wine_df = pd.read_csv('data/winequality-red.csv', delimiter=';')\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glance at the values of the quality of wine in the DataFrame\n",
    "wine_df.quality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dummies is a method called on the pandas module - you simply pass in a Pandas Series \n",
    "# or DataFrame, and it will convert a categorical variable into dummy/indicator variables. \n",
    "quality_dummies = pd.get_dummies(wine_df.quality, prefix='quality')\n",
    "quality_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the `join()` method. Remeber, this joins on indices by default. This means that we can simply join our quality dummies dataframe back to our original wine dataframe with the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = wine_df.join(quality_dummies)\n",
    "joined_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now look at concat. \n",
    "joined_df2 = pd.concat([quality_dummies, wine_df], axis=1)\n",
    "joined_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in a different data set, since we're looking at combining multiple data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red_wines_df = pd.read_csv('data/winequality-red.csv', delimiter=';')\n",
    "white_wines_df = pd.read_csv('data/winequality-white.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wines_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wines_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wines_quality_df = red_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\n",
    "red_wines_quality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wines_quality_df = white_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\n",
    "white_wines_quality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(red_wines_quality_df, white_wines_quality_df, on=['quality'], suffixes=[' red', ' white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "From [wiki](https://en.wikipedia.org/wiki/Pivot_table): \"Among other functions, a pivot table can automatically sort, count total, or give the average of the data stored in one table or spreadsheet, displaying the results in a second table showing the summarized data. Pivot tables are also useful for quickly creating unweighted cross tabulations.\"\n",
    "\n",
    "As you might have guessed, we have functionality to create pivot tables available for our use in Pandas. The way that we do this is by calling the `pivot_table()` function that is available on the pandas module (which we've stored as `pd`). As the [docs](http://pandas.pydata.org/pandas-docs/stable/reshaping.html#pivot-tables-and-cross-tabulations) tell us, the `pivot_table()` expects a number of different arguments: \n",
    "\n",
    "1. `data`: A DataFrame object\n",
    "2. `values`: a column or a list of columns to aggregate\n",
    "3. `index`: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.\n",
    "4. `columns`: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.\n",
    "5. `aggfunc`: function to use for aggregation, defaulting to numpy.mean\n",
    "\n",
    "Notice that by default this uses the mean for the `aggfunc` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's recall what the data looks like. \n",
    "red_wines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a moment to quickly learn about another Pandas function called `cut()` that allows us to turn a column with continuous data into categoricals by specifying bins to place them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.cut(red_wines_df['fixed acidity'], bins=np.arange(4, 17)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_acidity_bins = np.arange(4, 17)\n",
    "fixed_acidity_series = pd.cut(red_wines_df['fixed acidity'], bins=fixed_acidity_bins, \n",
    "                              labels=fixed_acidity_bins[:-1])\n",
    "fixed_acidity_series.name = 'fa_bin'\n",
    "red_wines_df = pd.concat([red_wines_df, fixed_acidity_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can get the mean residual sugar for each quality category/fixed acidity bin like we did earlier, but with a pivot_table (mean is the default agregation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(red_wines_df, values='residual sugar', index='quality', columns='fa_bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also specify a function to aggregate with\n",
    "pd.pivot_table(red_wines_df, values='residual sugar', index='quality', \n",
    "               columns='fa_bin', aggfunc=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
